{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"\ud83e\udde0 MEG/EEG GUI Software","text":"<p>Welcome to the documentation of DeepEpiX, a Dash-based M/EEG GUI Software. This app provides an interactive web interface for loading, preprocessing, annotating and analyzing raw M/EEG files, and last but not least: running prediction models.</p> <p></p>"},{"location":"#what-this-app-does","title":"\ud83d\ude80 What This App Does","text":"<ul> <li>\u2705 Load raw MEG/EEG datasets (<code>.ds</code>, <code>.fif</code>, or 4D-compatible)</li> <li>\u2705 Set frequency filtering parameters (resampling, high-pass, low-pass, notch)</li> <li>\u2705 Detect ECG peaks via channel hinting</li> <li>\u2705 Drop bad channels</li> <li>\u2705 Visualize event statistics, power spectral density, topomap, ICA...</li> <li>\u2705 Display temporal signal with various options</li> <li>\u2705 Create custom sensors layout (montage)</li> <li>\u2705 Run prediction models</li> <li>\u2705 Measure their performances</li> </ul>"},{"location":"#what-this-app-should-do-in-the-future","title":"\ud83e\udd14 What This App Should Do in the Future","text":"<ul> <li>\ud83d\udca1 Allow continuous learning of prediction models</li> </ul>"},{"location":"#app-structure","title":"\ud83d\uddc2 App Structure","text":"<p>The app is structured around pages, layout and callbacks.</p>"},{"location":"#docs-navigation","title":"\ud83d\udcd6 Docs Navigation","text":"<ul> <li>\ud83d\udc0b Fast Installation</li> <li>\ud83d\udc68\u200d\ud83d\udcbb Developer Setup</li> <li>\ud83e\udde9 First Tuto</li> </ul>"},{"location":"#who-is-this-for","title":"\ud83d\udc69\u200d\ud83d\udcbb Who Is This For?","text":"<ul> <li>Developers extending or maintaining the app</li> <li>Researchers and clinicians using the app for M/EEG studies</li> <li>Contributors improving UI, performance, or adding features</li> </ul>"},{"location":"app-structure/","title":"\ud83d\udcc2 The Dropdown Menu Explained","text":"<p>The application is divided into four main sections, each with its own set of pages.</p> <p></p>"},{"location":"folder-structure/","title":"\ud83d\uddc3\ufe0f Folder Structure","text":"<pre><code>DeepEpiX/\n\n\u251c\u2500\u2500 data/                 # Put your data here - when built with Docker, local data directory is mounted on it.\n\u2502   \u251c\u2500\u2500 patient_1.ds      \n\u2502   \u251c\u2500\u2500 patient_2.fif\n\u2502   \u251c\u2500\u2500 patient_3_4D/\n\u2502   \u2502   \u251c\u2500\u2500 rfDC_EEG\n\u2502   \u2502   \u251c\u2500\u2500 config\n\u2502   \u2502   \u2514\u2500\u2500 hs_file\n\n\u251c\u2500\u2500 docs/                 # Use mkdocs\n\n\u251c\u2500\u2500 requirements/         # Use pip-tools to generate .txt from .in\n\n\u251c\u2500\u2500 src/                  \n\u2502   \u251c\u2500\u2500 assets/           # Static image/logo/icons\n\u2502   \u251c\u2500\u2500 cache-directory/  # Cached intermediate data or results - cleaned every time a new subect is loaded\n\u2502   \u251c\u2500\u2500 callbacks/        # Contains chainable functions that are automatically called whenever a UI element on viz.py page is changed\n\u2502   \u2502   \u251c\u2500\u2500 utils/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 page1_utils.py \n\u2502   \u2502   \u2502   \u251c\u2500\u2500 page2_utils.py \n\u2502   \u2502   \u2502   \u251c\u2500\u2500 ...\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 pageN_utils.py\n\u2502   \u2502   \u251c\u2500\u2500 page1_callbacks.py \n\u2502   \u2502   \u251c\u2500\u2500 page2_callbacks.py \n\u2502   \u2502   \u251c\u2500\u2500 ...\n\u2502   \u2502   \u2514\u2500\u2500 page3_callbacks.py  \n\u2502   \u251c\u2500\u2500 layout/           # Contains UI elements definition\n\u2502   \u2502   \u251c\u2500\u2500 page1_layout.py \n\u2502   \u2502   \u251c\u2500\u2500 page2_layout.py \n\u2502   \u2502   \u251c\u2500\u2500 ...\n\u2502   \u2502   \u2514\u2500\u2500 pageN_layout.py  \n\u2502   \u251c\u2500\u2500 model_pipeline/   # Extracted from https://github.com/pmouches/DeepEpi/tree/main/pipeline with some modifications\n\u2502   \u251c\u2500\u2500 models/           # ML models from from https://github.com/pmouches/DeepEpi/tree/main/\n\u2502   \u251c\u2500\u2500 pages/            # Multi-page app\n\u2502   \u2502   \u251c\u2500\u2500 page1.py \n\u2502   \u2502   \u251c\u2500\u2500 page2.py \n\u2502   \u2502   \u251c\u2500\u2500 ...\n\u2502   \u2502   \u2514\u2500\u2500 pageN.py\n\u2502   \u251c\u2500\u2500 static/           # Static files\n\u2502   \u251c\u2500\u2500 config.py         # Configuration settings and constants\n\u2502   \u2514\u2500\u2500 run.py            # Entry point to run the multi-page app\n\n\n\u251c\u2500\u2500 DeepEpiX.def          # Singularity definition file for containerization\n\u251c\u2500\u2500 Dockerfile            # Docker definition file for containerization\n\u2514\u2500\u2500 README.md \n</code></pre> <p>This structure is schematic but aims to help you understand how the multi-page Dash app is organized.</p> <p>Each page is defined in the <code>pages/</code> directory:</p> <ul> <li>The layout of each page is declared in <code>layout/</code>.</li> <li>The interactivity (callbacks) is handled in <code>callbacks/</code>.</li> </ul> <p>\ud83d\udd0e Note: Since several pages share common callback functions, the callbacks, layout components, and utilities are organized by major components (e.g., <code>graph</code>, <code>history</code>, <code>ica</code>, <code>prediction</code>, <code>preprocessing</code>, etc.), rather than strictly by individual pages.</p>"},{"location":"install/","title":"\ud83d\udc0b Quick Start : Docker Installation (for Production Mode only)","text":"<p>Clone the repository in your working directory: <pre><code>git clone https://github.com/agnesgrd/DeepEpiX.git\n</code></pre></p> <p>Build and run the Docker container with your local data directory: <pre><code>cd DeepEpiX\ndocker build -t deepepix-app .\ndocker run -p 8050:8050 -v /home/user/DeepEpiX/data:/DeepEpiX/data deepepix-app # Modify this to point your local data path\n</code></pre></p> <p>Example of windows path to point to your data directory: <pre><code>docker run -p 8050:8050 -v //c/Users/pauli/Documents/MEGBase/data/exampleData:/DeepEpiX/data deepepix-app\n</code></pre></p> <p>Then, open the app in your web browser at: http://localhost:8050/</p> <p>You are ready to use DeepEpiX ! \ud83e\udd38\u200d\u2642\ufe0f</p>"},{"location":"install/#local-installation-for-development-mode","title":"\ud83d\udee0 Local Installation (for Development Mode)","text":"<p>\u21aa\ufe0f Go to Developer Guide &gt; Setup &amp; Run</p>"},{"location":"dev/callback/","title":"\ud83e\uddd1\u200d\ud83c\udfed Writing a Dash Callback","text":"<p>Dash callbacks define how interactive elements (like dropdowns, sliders, or buttons) update other components in your app.</p>"},{"location":"dev/callback/#basic-structure","title":"\ud83e\udde9 Basic Structure","text":"<pre><code>from dash import Input, Output, State, callback\n\n@callback(\n    Output(\"output-component-id\", \"property\"),\n    Input(\"input-component-id\", \"property\"),\n)\ndef update_output(input_value):\n    # Your logic here\n    return updated_value\n</code></pre>"},{"location":"dev/callback/#with-multiple-inputs-or-outputs-and-a-state","title":"\ud83d\udd01 With Multiple Inputs or Outputs and a State","text":"<pre><code>@callback(\n    [Output(\"graph\", \"figure\"), Output(\"text-box\", \"children\")],\n    [Input(\"dropdown\", \"value\"), Input(\"slider\", \"value\")],\n    State(\"parameter\", \"data\"),\n)\ndef update_graph_and_text(selected_option, slider_value, parameter):\n    fig = generate_figure(selected_option, slider_value, parameter)\n    message = f\"Slider is at {slider_value}\"\n    return fig, message\n</code></pre>"},{"location":"dev/callback/#key-concepts","title":"\ud83d\udccb Key Concepts","text":"Term Purpose <code>Input</code> Triggers the callback when the input changes <code>Output</code> The component property to be updated <code>State</code> Passes data to the function without triggering it"},{"location":"dev/callback/#development-notes","title":"\ud83d\uddd2\ufe0f Development Notes","text":"<ul> <li> <p>All Outputs must be returned in the same order they\u2019re defined.</p> </li> <li> <p>If necessary, use <code>prevent_initial_call=True</code> in <code>@callback</code> to skip execution on page load:</p> </li> </ul> <pre><code>@callback(..., prevent_initial_call=True)\n</code></pre> <ul> <li> <p>To avoid updating, either raise <code>dash.exceptions.PreventUpdate</code> exception to abort the whole callback, or return <code>dash.no_update</code> for each of the outputs that you do not wish to update:</p> <pre><code>from dash.exceptions import PreventUpdate\nif not input_value:\n    raise PreventUpdate\n</code></pre> <p>or </p> <pre><code>@callback(\n    Output(\"result\", \"children\"),\n    Input(\"submit-btn\", \"n_clicks\"),\n    State(\"input-text\", \"value\")\n)\ndef submit_text(n_clicks, input_value):\n    if n_clicks:\n        return f\"You submitted: {input_value}\"\n    return dash.no_update\n</code></pre> </li> </ul>"},{"location":"dev/callback/#resources","title":"\ud83d\udcda Resources","text":"<p>To better understand how callbacks work in Dash, see the official documentation:</p> <p>\ud83d\udd17 Dash Callbacks \u2013 Official Docs</p>"},{"location":"dev/new_page/","title":"\ud83d\udcc4 Adding a New Page","text":"<p>If you want to add a completely new feature to the app \u2014 for example, continuous learning of models \u2014 the easiest and cleanest way is to create a new, self-contained page.</p> <p>Since pages in this app do not directly communicate with one another (except through <code>dcc.Store</code> components), isolating your feature into its own page helps avoid unnecessary complexity or deep code digging.</p>"},{"location":"dev/new_page/#5-steps-to-add-a-new-feature-page","title":"\u2795 5 Steps to Add a New Feature Page","text":"<ol> <li> <p>Create the page file.    Add a new file named <code>new_page.py</code> in the <code>pages/</code> directory.</p> </li> <li> <p>Register the page.    Inside the new file, include the following line:    <pre><code>dash.register_page(__name__, name = \"New page\", path=\"/path/to/the/new/page\")\n</code></pre></p> </li> <li> <p>Define the layout.    Create a variable or function named layout that returns the page\u2019s content:    <pre><code>layout = html.Div([  \n   #Your page components here \n])\n</code></pre></p> </li> <li> <p>Define the callbacks.     Write callback functions that dynamically update your page components.     Use the following pattern to register callbacks:     Call this registration function inside <code>new_page.py</code>. <pre><code># callbacks/your_feature.py\ndef register_new_callbacks(**your_args):\n   @callback(\n      Output(component1, 'property1'),\n      Output(component2, 'property2'),\n      Input(component3, 'property3'),\n      State(component4, 'property4')\n   )\n   def _new_callbacks(input_value, state_value):\n      # Your logic here\n      return updated_value1, updated_value2\n</code></pre></p> </li> <li> <p>Update run.py (main app file).    The app is already initialized with multi-page support:    <pre><code>app = Dash(__name__, use_pages=True)\n</code></pre>    Ensure the new page is included using dash.page_container.    You can also control the page's order or placement in the DropdownMenu component from here.</p> </li> </ol> <p>\ud83d\udca1 Best Practice: Group related callbacks into separate files inside the <code>callbacks/</code> directory. Structure them by feature or component (e.g., <code>graph.py</code>, <code>prediction.py</code>), not strictly by page.</p>"},{"location":"dev/new_page/#resources","title":"\ud83d\udcda Resources","text":"<p>To better understand how multi-page apps work in Dash, see the official documentation:</p> <p>\ud83d\udd17 https://dash.plotly.com/urls</p>"},{"location":"dev/setup/","title":"Setup & run","text":""},{"location":"dev/setup/#local-installation-for-development-mode","title":"\ud83d\udee0 Local Installation (for Development Mode)","text":"<ol> <li> <p>Clone the Repository in your working directory: <pre><code>git clone https://github.com/agnesgrd/DeepEpiX.git\n</code></pre></p> </li> <li> <p>Set up the Dash Environment: <pre><code>cd DeepEpiX\npython3 -m venv .dashenv\nsource .dashenv/bin/activate\npython3 -m pip install -r requirements/requirements-dashenv.txt\ndeactivate\n</code></pre></p> </li> <li> <p>Set up Prediction Model Environments: <pre><code>python3 -m venv .tfenv\nsource .tfenv/bin/activate\npython3 -m pip install -r requirements/requirements-tfenv.txt\ndeactivate\n</code></pre> <pre><code>python3 -m venv .torchenv\nsource .torchenv/bin/activate\npython3 -m pip install -r requirements/requirements-torchenv.txt\ndeactivate\n</code></pre></p> </li> <li>Activate your Dash Environment and Start Running the App: <pre><code>source .dashenv/bin/activate\npython3 src/run.py\n</code></pre> Then, open the app in your web browser at: http://localhost:8050/</li> </ol> <p>\ud83e\udd73 You can start editing code while visualizing automatic reloads (if DEBUG is set to True in <code>config.py</code>). </p> <p>For quick access, ensure that your M/EEG data is placed in the data folder within the project directory.</p>"},{"location":"dev/setup/#development-notes","title":"\ud83d\uddd2\ufe0f Development Notes","text":"<ul> <li> <p>\ud83d\udc0d Python Version:   DeepEpiX was developed using Python 3.9. It is recommended to use this version to ensure compatibility.</p> </li> <li> <p>\ud83e\uddf0 Recommended Developer Tools:</p> <ul> <li> <p><code>pip-tools</code>:     Helps manage and compile <code>requirements.in</code> into a clean, version-pinned <code>requirements.txt</code>.     <pre><code>pip install pip-tools\npip-compile --output-file=requirements.txt requirements.in\n</code></pre></p> </li> <li> <p><code>mkdocs</code>:     Used to build clean, static documentation sites from Markdown files.     <pre><code>pip install mkdocs\nmkdocs serve  # Preview the docs locally\nmkdocs build  # Generate the static site\n</code></pre></p> </li> </ul> </li> </ul>"},{"location":"dev/subprocess/","title":"\u2699\ufe0f Running External Scripts with <code>subprocess</code> in Python","text":"<p>This section describes how to run a Python script in a separate virtual environment using the <code>subprocess</code> module. This approach is useful when models or pipelines require isolated environments like TensorFlow or PyTorch.</p>"},{"location":"dev/subprocess/#use-case-executing-a-model-in-a-separate-virtual-environment","title":"\ud83d\udcc1 Use Case: Executing a Model in a Separate Virtual Environment","text":"<p>To simplify development and reduce dependency conflicts, we separated the Dash environment from heavy ML frameworks like TensorFlow and PyTorch:</p> <ul> <li> <p>The Dash app runs inside a lightweight environment:  </p> <ul> <li><code>.dashenv</code></li> </ul> </li> <li> <p>The model listed in <code>models/</code> require dedicated ML environments:</p> <ul> <li><code>.tfenv</code></li> <li><code>.torchenv</code></li> </ul> </li> </ul> <p>When the user decides to run a model, the app should detect the required backend and delegate execution to the appropriate Python binary.</p> <p>Below is an example script we want to run using a specific ML environment (<code>model_pipeline/run_model.py</code>).</p> <pre><code># model_pipeline/run.model.py\nfrom ... import run_model_pipeline\n\nif __name__ == \"__main__\":\n    model_path = sys.argv[1]\n    model_type = sys.argv[2]\n    data_path = sys.argv[3]\n    results_path = sys.argv[4]\n    threshold = float(sys.argv[5])\n    adjust_onset = sys.argv[6]\n    bad_channels = sys.argv[7]\n\n    run_model_pipeline(model_path, model_type, data_path, results_path, threshold, adjust_onset, bad_channels)\n</code></pre>"},{"location":"dev/subprocess/#step-by-step-example","title":"\u2705 Step-by-Step Example","text":"<ol> <li>Classical Import <pre><code># callbacks/predict_callbacks.py\nimport subprocess\nimport os\nimport time\nfrom pathlib import Path\n</code></pre></li> <li> <p>Backend Detection</p> <p><pre><code>    # Select the Python executable based on the virtual environment\n    if \"TensorFlow\" in venv:\n        ACTIVATE_ENV = str(config.TENSORFLOW_ENV / \"bin/python\")      \n    elif \"PyTorch\" in venv:\n        ACTIVATE_ENV = str(config.TORCH_ENV / \"bin/python\")\n</code></pre> The variable <code>venv</code> is determined earlier in the function based on the model file's extension (<code>.pth</code>, .<code>keras</code>, <code>h5</code>). <code>TENSORFLOW_ENV</code> and <code>TORCH_ENV</code> are paths to the respective virtual environments, defined in the <code>config</code> module.</p> </li> <li> <p>Command Definition</p> <p>The command should be passed as a list of strings, where each list element is a separate argument. <pre><code>    # Build the command to execute\n    command = [\n        ACTIVATE_ENV,\n        \"model_pipeline/run_model.py\",         # Script to run\n        str(model_path),                       # Argument 1\n        str(venv),                             # Argument 2\n        str(data_path),              # Argument 3\n        str(cache_dir),                        # Argument 4\n        str(threshold),                        # Argument 5\n        str(adjust_onset),                     # Argument 6\n        str(channel_store.get('bad', []))      # Argument 7\n    ]\n</code></pre></p> </li> <li> <p>Python Path</p> <p>If the <code>PYTHONPATH</code> environment variable isn't set correctly when using <code>subprocess</code>, Python may not be able to locate local modules properly, leading to import failures.</p> <pre><code>    # Set the working directory and environment variables\n    working_dir = Path.cwd()\n    env = os.environ.copy()\n    env[\"PYTHONPATH\"] = str(working_dir)  # Ensure imports work correctly\n</code></pre> </li> <li> <p>Subprocess execution <pre><code>    # Run the subprocess\n    try:\n        subprocess.run(command, env=env, text=True)  # Run command in isolated process\n\n    except Exception as e:\n        print(f\"\u26a0\ufe0f Error running model: {e}\")\n</code></pre>     Use <code>text=True</code> to ensure output is treated as text (not bytes).     To suppress output, uncomment <code>stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL</code>.</p> </li> </ol>"},{"location":"dev/subprocess/#tips","title":"\ud83d\udca1 Tips","text":"<ul> <li> <p>To retrieve model results afterward, make sure your script saves outputs (e.g., CSVs, logs) into the <code>cache-directory</code>/.</p> </li> <li> <p>Async option: use <code>multiprocessing</code> or <code>ThreadPoolExecutor</code> if you don\u2019t want the Dash app to freeze during long model runs.</p> </li> </ul>"},{"location":"dev/subprocess/#when-to-use-this","title":"\ud83d\udcd8 When to Use This","text":"<p>Use a subprocess when:</p> <ul> <li> <p>You want to isolate dependencies (e.g., TensorFlow vs. PyTorch).</p> </li> <li> <p>You need to run long or blocking processes separately.</p> </li> </ul>"},{"location":"user/dataset/","title":"Tuto 1 : Load &amp; Preprocess Data","text":""},{"location":"user/dataset/#1-go-to-home-page","title":"1\ufe0f\u20e3 Go to Home page","text":"<p>Open the app as explained in the Setup &amp; Installation section. You should arrive at the \ud83c\udfe0 Home page. If not, use the \u2630 menu at the top left to navigate there.</p>"},{"location":"user/dataset/#2-choose-a-subject","title":"2\ufe0f\u20e3 Choose a Subject","text":"<p>Use the \ud83d\udd3d dropdown menu to select a subject.  </p> <p>Note: The \ud83d\udcc2 Open Folder button only works if you have installed DeepEpiX locally.</p> <p>You can open the following types of files:</p> <ul> <li><code>.ds</code> folders</li> <li><code>.fif</code> files</li> <li><code>4D</code> folders (must include at least: <code>rfDC-EEG</code>, <code>config</code> and <code>hs-file</code>)</li> </ul>"},{"location":"user/dataset/#3-load-and-access-metadata","title":"3\ufe0f\u20e3 Load and Access Metadata","text":"<p>When you click on \ud83d\udce5 Load, the previous database memory is cleared and \u2699\ufe0f preprocessing parameters become accessible. You can adjust these parameters while exploring:</p> <ul> <li>Metadata (<code>raw.info</code>)</li> <li>Past annotations </li> <li>Power Spectrum Decomposition (as a function of frequency parameters)</li> </ul>"},{"location":"user/dataset/#4-preprocess-and-visualize","title":"4\ufe0f\u20e3 Preprocess and Visualize","text":"<p>In this step, the following preprocessing operations are applied:</p> <ul> <li>Resampling</li> <li>Band-pass filtering</li> <li>Notch filtering (to remove line noise)</li> </ul>"},{"location":"user/dataset/#optional-settings","title":"Optional Settings","text":"<ul> <li> <p>\ud83d\udc93 ECG Channel Detection</p> <p>Specify a channel that clearly captures the heartbeat for ECG event detection using <code>mne.find_ecg_events</code>. Default: <code>None</code> (all channels are used)</p> </li> <li> <p>\u274c Bad Channels</p> <p>Specify channels to exclude from analysis (e.g., topographic plots, model predictions), while still allowing them to be visualized.  These channels will be marked as bad and grouped accordingly. Accepts a single channel name or a comma-separated list.  These will be added to the existing bad channels in <code>raw.info</code>. If desired, they can be saved with the new annotations at the end of your session.</p> </li> </ul> <p>Tip: To find the correct channel name format, check section \ud83d\uddc2\ufe0f Raw Info \u2192 Channel Names.</p> <p>Clicking \u26a1 Preprocess will:</p> <ul> <li>Filter and resample the data</li> <li>Store it in memory for the session duration</li> <li>Take you to the \ud83d\udcc8 Raw Visualization main page.</li> </ul> <p>To view metadata again, return to the \ud83c\udfe0 Home page and check the \ud83d\udcda Database table.</p> <p></p>"},{"location":"user/ica-viz/","title":"Tuto 6: Visualize ICA Components","text":""},{"location":"user/ica-viz/#1-go-to-the-ica-visualization-page","title":"1\ufe0f\u20e3 Go to the \ud83d\udcc8 ICA Visualization page.","text":"<p>If the left sidebar is collapsed, click the  Open Sidebar button. The sidebar contains 2 tabs:</p> <ul> <li> Compute</li> <li> Select</li> </ul>"},{"location":"user/ica-viz/#2-compute-ica-with-the-method-of-your-choice","title":"2\ufe0f\u20e3 Compute ICA with the method of your choice","text":"<p>In the  Compute tab:</p> <ul> <li>Enter the Number of Components (<code>n-components</code>).</li> <li>Choose an ICA Method (<code>fastica</code>, <code>infomax</code>, or <code>picard</code>).</li> <li>Set Max Iterations (<code>max-iter</code>).</li> <li>Set Temporal Decimation (<code>decim</code>).</li> <li>Click the \u26a1 Compute button to start ICA fitting.</li> </ul> <p>The ICA solution computed with <code>mne.preprocessing.ICA(...)</code> method is saved in cache as a <code>.fif</code> file. Progress and past computations appear in the ICA History log.</p>"},{"location":"user/ica-viz/#3-select-ica-result-and-set-your-display-preferences","title":"3\ufe0f\u20e3 Select ICA result and set your display preferences","text":"<p>In the  Select tab:</p> <ul> <li>Pick an ICA result.</li> <li>Enable annotations to display on the main graph and in the annotation overview (below the time slider).</li> <li>Set amplitude (1\u201310) to adjust signal scaling (affects how compressed or expanded the signal appears vertically).</li> <li>Pick a color scheme (e.g., rainbow applies group-based coloring).</li> <li>Click the \ud83d\udd04 Refresh button on the top-left Modebar to apply changes.</li> </ul>"},{"location":"user/ica-viz/#4-once-the-graph-is-displayed","title":"4\ufe0f\u20e3 Once the Graph is Displayed","text":"<p>Left Modebar:</p> <ul> <li>\ud83d\udd04 Refresh the graph after changing ICA result, amplitude, or color settings.</li> <li>\u23e9 Navigate between pages (the signal is displayed in 2-minute chunks for performance; this duration can be modified in config.py).</li> <li>\ud83e\udded Jump to previous/next event beyond the current view (default: all selected events; can be filtered by event type).</li> </ul> <p>Right Modebar:</p> <ul> <li>\ud83d\udcf8 Take a snapshot of the current view.</li> <li>\ud83d\udd0d Zoom in on time and components.</li> <li>\ud83d\uddb1\ufe0f Pan horizontally by clicking and dragging.</li> <li>\u23f1\ufe0f Zoom in/out on the time axis.</li> <li>\ud83e\uddfc Autoscale or reset to display the full signal duration.</li> </ul> <p>Time Range Slider:</p> <ul> <li>Navigate through the signal timeline.</li> <li>Adjust the visible time range.</li> </ul> <p>Component Slider:</p> <ul> <li>Use your mouse or trackpad to scroll through components vertically.</li> </ul> <p>\ud83d\udcdd Annotation Overview</p> <ul> <li>View annotation positions below the graph for quick reference.</li> </ul>"},{"location":"user/montage/","title":"Tuto 2: Create New Sensors Layout (Montage)","text":"<p>What is a montage? </p> <p>For clinicians, creating custom sensor montages is essential for efficient analysis. Rather than viewing all available channels, a montage allows you to focus on specific brain regions of interest, making it easier to identify and analyze neural activity patterns relevant to your clinical assessment.</p> <p></p>"},{"location":"user/montage/#1-choose-a-name","title":"1\ufe0f\u20e3 Choose a Name","text":"<p>Click on the \u2795 Create button to begin.</p>"},{"location":"user/montage/#2-choose-a-selection-method","title":"2\ufe0f\u20e3 Choose a Selection Method","text":"<p>You have two options:</p> \ud83d\uddb1\ufe0f Choose Sensors Manually\ud83c\udfb2 Choose with Random Pick <p>Select the sensors you want one by one.</p> <p>Apply a % percentage on all groups and/or adjust counts directly for each group</p> <p>You can see the results live on the left side.</p> <p>\ud83d\udcda About groups: Sensor groups are defined based on anatomical brain regions as described in [paper references to be added].</p>"},{"location":"user/montage/#3-save-and-locate","title":"3\ufe0f\u20e3 Save and Locate","text":"<p>After you're satisfied, \ud83d\udcbe save your montage locally.</p> <p>You can now find it in the \ud83d\uddc2\ufe0f Montage Table or under \ud83e\udde0 Select Montage in the visualization pages.</p> <p>\u26a0\ufe0f Note: A montage is specific to the channel name format (e.g., <code>MRC22-2805</code> \u2260 <code>A22</code>) and to the available channels of the patient loaded at the time of creation. This explains why a selected montage may not work properly during visualization, even when the channel formats appear similar.</p>"},{"location":"user/montage/#4-manage-your-montage","title":"4\ufe0f\u20e3 Manage Your Montage","text":"<p>You can easily delete a montage by clicking on the \ud83d\uddd1\ufe0f trash icon next to it.</p>"},{"location":"user/new-model/","title":"Tuto 6: Add your own model","text":"<p>This tutorial explains how to integrate a custom model into the Dash app. Models are executed in separate environments to avoid dependency conflicts and to keep the app lightweight.</p> <p>You need to install the app locally because it requires code adjustments to make your custom model pipeline work.  \u21aa\ufe0f Go to Developer Guide &gt; Setup &amp; Run</p>"},{"location":"user/new-model/#folder-structure","title":"Folder Structure","text":"<ul> <li> <p>Dash app environment: <code>.dashenv</code> \u2192 Runs the Dash UI and callbacks.  </p> </li> <li> <p>Model environments: <code>.tfenv</code> \u2192 TensorFlow models (<code>.keras</code>, <code>.h5</code>) <code>.torchenv</code> \u2192 PyTorch models (<code>.pth</code>)  </p> </li> <li> <p>Model folder:   Place your model inside <code>models/</code> with the correct extension.</p> </li> </ul>"},{"location":"user/new-model/#how-it-works","title":"How It Works","text":"<p>When you run a model:</p> <ol> <li>The app detects the model type based on the file extension.</li> <li>It selects the corresponding Python binary (<code>.tfenv</code> or <code>.torchenv</code>).</li> <li>A subprocess is started to execute the model script (<code>model_pipeline/run_model.py</code>).</li> <li>Results are saved to the cache directory for later retrieval.</li> </ol>"},{"location":"user/new-model/#adding-your-model","title":"\ud83d\ude80 Adding Your Model","text":"<p>1. Place Your Model</p> <p>Add your trained model file to the <code>models/</code> directory:</p> <ul> <li>TensorFlow \u2192 <code>.keras</code> or <code>.h5</code> </li> <li>PyTorch \u2192 <code>.pth</code></li> </ul> <p>2. If needed, update <code>config.py</code></p> <p>Define the paths to your virtual environments: <pre><code>TENSORFLOW_ENV = Path(\"/path/to/.tfenv\")\nTORCH_ENV = Path(\"/path/to/.torchenv\")\n</code></pre> If you created your environments following the setup tutorial, you don\u2019t need to modify anything here. You may, however, need to install additional packages inside the TensorFlow or Pytorch environment depending on your model requirements.</p> <p>3. Inference Pipeline</p> <p>The app launches inference depending on whether the model is PyTorch or TensorFlow. Originally, it uses our preprocessing functions (<code>save_data_matrices</code>, <code>create_windows</code>, <code>generate_database</code>) before running inference. By default, inference is delegated to <code>test_model_dash</code> from the respective backend.</p> <p>If your model requires a custom pipeline, you can extend <code>run_model_pipeline</code> by adding conditions such as:</p> <pre><code>if model_name == \"path/to/your/custom/model\":\n    return test_model_custom(...)\n</code></pre> <p>argument it can takes and format final output</p>"},{"location":"user/new-model/#checklist-before-running","title":"\u2705 Checklist Before Running","text":"<ul> <li>Model file is in <code>models/</code>.</li> <li>Virtual environments are correctly defined in <code>config.py</code>.</li> <li><code>run_model_pipeline</code> can handle your model type, or has been extended with your custom logic.</li> <li>Script writes results (CSV, logs, etc.) to the cache directory.</li> </ul>"},{"location":"user/raw-analyze/","title":"Tuto 4: Analyze Preprocessed Signal","text":""},{"location":"user/raw-analyze/#1-visualize-preprocessed-signal","title":"1\ufe0f\u20e3 Visualize preprocessed signal","text":"<p>See Tuto</p>"},{"location":"user/raw-analyze/#2-analyze-your-signal-manually","title":"2\ufe0f\u20e3 Analyze Your Signal Manually","text":"<p>In the  Analyze tab:</p> <ul> <li>Activate the Topomap: click once on the button to activate the feature (click again to deactivate). Then click on the signal at the desired timepoint to view spatial maps.</li> <li>Add annotations: click or select a segment on the graph to auto-fill onset and duration (set duration = 0 for punctual events). Add a label and click Add New Event. It will appear in the annotation options (make sure it is selected to be visible on the graph).</li> <li>Delete annotations: select a segment and click Deleted Selected Event(s) to remove the current event(s) in the selection.</li> <li>To delete an entire event category, go to the Select tab, choose the event type, and click Delete, then confirm.</li> </ul>"},{"location":"user/raw-analyze/#3-run-a-prediction-model","title":"3\ufe0f\u20e3 Run a Prediction Model","text":"<p>In the  Spike Prediction tab:</p> <ul> <li>Select your model.</li> <li>Optional: enable sensitivity analysis and onset adjustement.<ul> <li>Sensitivity analysis (SmoothGrad) averages gradients over noisy inputs to highlight the most influential regions of the signal (available for simple TensorFlow models).</li> <li>You can also choose GFP-based alignment, which adjusts the spike onset to the peak of Global Field Power for greater accuracy.</li> </ul> </li> <li>Click Run Prediction. It should complete in under a minute (on GPU).</li> <li>When results appear:<ul> <li>Adjust the threshold to refine spike detection based on the displayed probability distribution.</li> <li>Rename the result (default: model_name_threshold_value) and click Save to make it selectable in the graph view.</li> </ul> </li> </ul> <p>If SmoothGrad was selected, a new color layer will be added in the Select panel. It highlights predicted spike regions on the signal.</p> <p>Running the same model again for the same subject will return cached results instantly\u2014you can re-save with a different threshold.</p>"},{"location":"user/raw-viz/","title":"Tuto 3: Visualize Preprocessed Signal","text":""},{"location":"user/raw-viz/#1-go-to-the-raw-visualization-main-page","title":"1\ufe0f\u20e3 Go to the \ud83d\udcc8 Raw Visualization main page","text":"<p>If the left sidebar is collapsed, click the  Open Sidebar button. The sidebar contains 4 tabs:</p> <ul> <li> Select</li> <li> Analyze</li> <li> Spike Prediction</li> <li> Save</li> </ul>"},{"location":"user/raw-viz/#2-set-your-display-preferences","title":"2\ufe0f\u20e3 Set Your Display Preferences","text":"<p>In the  Select tab:</p> <ul> <li>Choose a montage (channel layout), or if \"channel selection\" is active, pick specific sensor groups to view.</li> <li>Enable annotations to display on the main graph and in the annotation overview (below the time slider).</li> <li>Set amplitude (1\u201310) to adjust signal scaling (affects how compressed or expanded the signal appears vertically).</li> <li>Pick a color scheme (e.g., rainbow applies group-based coloring).</li> <li>Click the \ud83d\udd04 Refresh button on the top-left Modebar to apply changes.</li> </ul>"},{"location":"user/raw-viz/#3-once-the-graph-is-displayed","title":"3\ufe0f\u20e3 Once the Graph is Displayed","text":"<p>Left Modebar:</p> <ul> <li>\ud83d\udd04 Refresh the graph after changing channels, amplitude, or color settings.</li> <li>\u23e9 Navigate between pages (the signal is displayed in 2-minute chunks for performance; this duration can be modified in config.py).</li> <li>\ud83e\udded Jump to previous/next event beyond the current view (default: all selected events; can be filtered by event type).</li> </ul> <p>Right Modebar:</p> <ul> <li>\ud83d\udcf8 Take a snapshot of the current view.</li> <li>\ud83d\udd0d Zoom in on time and channels.</li> <li>\ud83d\uddb1\ufe0f Pan horizontally by clicking and dragging.</li> <li>\u23f1\ufe0f Zoom in/out on the time axis.</li> <li>\ud83e\uddfc Autoscale or reset to display the full signal duration.</li> </ul> <p>Time Range Slider:</p> <ul> <li>Navigate through the signal timeline.</li> <li>Adjust the visible time range.</li> </ul> <p>Channel Slider:</p> <ul> <li>Use your mouse or trackpad to scroll through channels vertically.</li> </ul> <p>\ud83d\udcdd Annotation Overview</p> <ul> <li>View annotation positions below the graph for quick reference.</li> </ul>"},{"location":"user/save/","title":"Tuto 5: Save your modifications","text":""},{"location":"user/save/#1-go-to-the-raw-visualization-main-page","title":"1\ufe0f\u20e3 Go to the \ud83d\udcc8 Raw Visualization main page","text":"<p>If the left sidebar is collapsed, click the  Open Sidebar button. The sidebar contains 4 tabs:</p> <ul> <li> Select</li> <li> Analyze</li> <li> Spike Prediction</li> <li> Save</li> </ul>"},{"location":"user/save/#2-save-your-modifications","title":"2\ufe0f\u20e3 Save Your modifications","text":"<p>In the  Save tab:</p> <ul> <li>Choose the annotations and bad channels you want to keep.</li> <li>Select a saving format:<ul> <li><code>.fif</code> is recommended.</li> <li>original: specific to <code>.ds</code> format, saves only the new marker file.</li> </ul> </li> </ul>"}]}